Folder Structure:
|-- GeminiAgent/
  |-- llm_api.py
  |-- main.py
  |-- archive/
  |-- output/
  |-- agents/
    |-- math_agent.py
    |-- language_agent.py
    |-- __init__.py
    |-- agent.py
  |-- utilities/
  |-- prompts/
  |-- myenv/
  |-- .git/

Code Blocks:

llm_api.py
```python
import os
import json
from dotenv import load_dotenv
import google.generativeai as genai

# Load the environment variables from the .env file
load_dotenv()

# Configure the API with your API key stored in an environment variable
genai.configure(api_key=os.environ["API_KEY"])

# Create a model instance
model = genai.GenerativeModel('gemini-1.0-pro-latest')

def call_google_llm_api(prompt):
    try:
        response = model.generate_content(prompt)
        # Convert the raw text response to JSON format
        json_response = json.dumps({"response": response.text})
        return json_response
    except Exception as e:
        print(f"An error occurred: {e}")
        return None
```

main.py
```python
import json
from agents.math_agent import MathAgent
from agents.language_agent import LanguageAgent
from llm_api import call_google_llm_api

# Load prompts from files
with open("prompts/master_agent_prompt.txt", "r") as file:
    master_agent_prompt = file.read()

with open("prompts/math_agent_prompt.txt", "r") as file:
    math_agent_prompt = file.read()

with open("prompts/language_agent_prompt.txt", "r") as file:
    language_agent_prompt = file.read()

# Predefined function table and agent instances
function_table = {
    "calculate": lambda expression: eval(expression),
    "translate": lambda text, target_lang: f"Translating '{text}' to {target_lang}"
}

math_agent = MathAgent("Math Agent", "Mathematics", function_table, math_agent_prompt)
language_agent = LanguageAgent("Language Agent", "Language", function_table, language_agent_prompt)

agent_registry = {
    "Math Agent": math_agent,
    "Language Agent": language_agent
}

# Conversation loop
conversation_history = ""
user_input = input("User: ")

while True:
    # Update the conversation history and user input in the master agent's prompt
    prompt = master_agent_prompt.replace("<conversation_history>", conversation_history)
    prompt = prompt.replace("<user_input>", user_input)

    # Send the master agent prompt to the Google LLM API and get the response
    master_agent_response = call_google_llm_api(prompt)

    if master_agent_response is None:
        print("An error occurred while communicating with the LLM API.")
        break

    print("Master Agent Response:", master_agent_response)

    try:
        # Parse the JSON response
        response_dict = json.loads(master_agent_response)
        response_text = response_dict["response"]
        print("MASTER Response Text:", response_text)
        
        # Parse the response text as JSON
        delegations_dict = json.loads(response_text)
        delegations = delegations_dict["delegations"]
    except (json.JSONDecodeError, KeyError) as e:
        print("Error parsing response:", e)
        break

    # Execute the delegations
    for delegation in delegations:
        agent_name = delegation["agent"]
        task = delegation["task"]
        agent = agent_registry[agent_name]
        response = agent.generate_response(task)
        conversation_history += f"{agent_name}: {response}\n"

    # Check if all delegations have been completed
    prompt = f"""
Given the following conversation history:

{conversation_history}

Determine if all the delegated tasks have been completed and provide a final response to the user. If the tasks are not yet completed, indicate what additional information or tasks are needed.

Please respond with a well-formatted JSON object using the following structure:
{{
  "tasks_completed": true or false,
  "final_response": "Final response to the user if tasks are completed",
  "additional_info_needed": "Additional information or tasks needed if not completed"
}}

Ensure that the JSON object is complete and properly formatted, without any extra text, formatting characters, or code blocks.

JSON Response:
"""

    master_agent_response = call_google_llm_api(prompt)

    # To check if it can respond correctly
    print(f"MASTER AGENT RESPONSE: {master_agent_response}")

    if master_agent_response is None:
        print("An error occurred while communicating with the LLM API.")
        break

    try:
        # Parse the JSON response
        response_dict = json.loads(master_agent_response)
        response_text = response_dict["response"]
        
        # Remove any extra text, formatting characters, or code blocks
        response_text = response_text.strip()
        response_text = response_text.replace("\`\`\`json", "").replace("\`\`\`", "")
        response_text = response_text.strip()

        if response_text.startswith("{") and response_text.endswith("}"):
            result_dict = json.loads(response_text)
            tasks_completed = result_dict.get("tasks_completed", False)
            final_response = result_dict.get("final_response", "")
            additional_info_needed = result_dict.get("additional_info_needed", "")
        else:
            raise json.JSONDecodeError("Invalid JSON format", response_text, 0)

        if tasks_completed:
            conversation_history += f"Master Agent: {final_response}\n"
            print(final_response)
        else:
            conversation_history += f"Master Agent: {additional_info_needed}\n"
            print(additional_info_needed)
    except (json.JSONDecodeError, KeyError) as e:
        print("Error parsing response:", e)
        conversation_history += "Master Agent: I apologize, but I couldn't generate a proper response. Can you please try rephrasing your request?\n"
        print("I apologize, but I couldn't generate a proper response. Can you please try rephrasing your request?")

    print(f"Convo History: {conversation_history}")

    # Check if the user wants to end the conversation
    if user_input.lower() in ["bye", "goodbye", "exit", "quit"]:
        break

    # Get the user's input for the next turn
    user_input = input("User: ")

# End the conversation
print("Master Agent: Thank you for the conversation. Have a great day!")
```

math_agent.py
```python
import ast
import operator
import math
import json
from .agent import Agent
from llm_api import call_google_llm_api

class MathExpressionEvaluator:
    def __init__(self):
        self.operators = {
            ast.Add: operator.add,
            ast.Sub: operator.sub,
            ast.Mult: operator.mul,
            ast.Div: operator.truediv,
            ast.Pow: operator.pow
        }
        self.functions = {
            'sqrt': math.sqrt,
            'log': math.log,
            'log10': math.log10,
            'log2': math.log2,
            'exp': math.exp,
            'sin': math.sin,
            'cos': math.cos,
            'tan': math.tan,
            'factorial': math.factorial,
            'pow': math.pow  # Add the 'pow' function
        }

    def evaluate(self, expression):
        try:
            tree = ast.parse(expression, mode='eval')
            return self.eval_node(tree.body)
        except SyntaxError:
            # If the expression contains the 'pow()' function, replace it with '**' and try again
            if 'pow(' in expression:
                expression = expression.replace('pow(', '(').replace(',', '**')
                tree = ast.parse(expression, mode='eval')
                return self.eval_node(tree.body)
            else:
                raise ValueError("Invalid mathematical expression")

    def eval_node(self, node):
        if isinstance(node, ast.Num):
            return node.n
        elif isinstance(node, ast.BinOp):
            left = self.eval_node(node.left)
            right = self.eval_node(node.right)
            return self.operators[type(node.op)](left, right)
        elif isinstance(node, ast.UnaryOp):
            operand = self.eval_node(node.operand)
            return self.operators[type(node.op)](operand)
        elif isinstance(node, ast.Call):
            func_name = node.func.id
            if func_name not in self.functions:
                raise ValueError(f"Unknown function: {func_name}")
            args = [self.eval_node(arg) for arg in node.args]
            return self.functions[func_name](*args)
        else:
            raise TypeError(f"Unsupported node type: {type(node)}")

class MathAgent(Agent):
    def __init__(self, name, expertise, function_table, prompt):
        super().__init__(name, expertise, function_table, prompt)
        self.expression_evaluator = MathExpressionEvaluator()

    def generate_response(self, task):
        # Generate a response for math-related tasks
        if "calculate" in task.lower():
            # Convert the natural language task to a mathematical expression
            expression = self.convert_to_expression(task)
            print(f"EXPRESSION: {expression}")

            if expression:
                try:
                    # Evaluate the expression using the custom expression evaluator
                    result = self.expression_evaluator.evaluate(expression)
                    print(f"RESULT: {result}")  # Debugging message
                    return f"The result of {expression} is: {result}"
                except (ValueError, TypeError) as e:
                    print(f"ERROR: {str(e)}")  # Debugging message
                    return str(e)
            else:
                return "I couldn't extract a valid mathematical expression from the task. Please rephrase it."
        else:
            return "I can assist with mathematical calculations. Please provide a valid expression."

    def convert_to_expression(self, task):
        # Call the Google LLM API to convert the natural language task to a mathematical expression
        prompt = f"""
        Convert the following natural language task to a mathematical expression that can be evaluated using the math library in Python. Respond with a JSON object containing the expression:
        Task: {task}
        Respond with a JSON object in the following format:
        {{ "expression": "<mathematical_expression>" }}
        Make sure to include only the JSON object in your response, without any additional text or explanations.
        Examples:
        Task: Calculate the square root of 64
        Response: {{ "expression": "sqrt(64)" }}
        Task: Calculate the natural logarithm of 10
        Response: {{ "expression": "log(10)" }}
        Task: Calculate log base 10 of 100
        Response: {{ "expression": "log10(100)" }}
        JSON Response:
        """
        response = call_google_llm_api(prompt)

        if response:
            try:
                response_data = json.loads(response)
                expression_data = json.loads(response_data["response"])
                return expression_data["expression"]
            except (json.JSONDecodeError, KeyError):
                return None
        else:
            return None
```

language_agent.py
```python
from .agent import Agent

class LanguageAgent(Agent):
    def generate_response(self, task):
        # Generate a response for language-related tasks
        if "translate" in task.lower():
            # Extract the text and target language from the task and translate
            parts = task.split("translate", 1)[1].strip().split("to")
            text = parts[0].strip().strip("'\"")
            target_lang = parts[1].strip()
            translated_text = self.execute_function("translate", text, target_lang)
            return f"The translation of '{text}' to {target_lang} is: {translated_text}"
        else:
            return "I can assist with language translation. Please provide the text and target language."
```

__init__.py
```python

```

agent.py
```python
class Agent:
    def __init__(self, name, expertise, function_table, prompt):
        self.name = name
        self.expertise = expertise
        self.function_table = function_table
        self.prompt = prompt

    def execute_function(self, function_name, *args, **kwargs):
        if function_name in self.function_table:
            return self.function_table[function_name](*args, **kwargs)
        else:
            raise ValueError(f"Function '{function_name}' not found in the function table")

    def generate_response(self, task):
        # Process the task and generate a response
        # This method can be overridden by specialized agents
        return "I'm not sure how to handle this task."
```

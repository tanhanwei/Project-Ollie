Folder Structure:
|-- GeminiAgent/
  |-- llm_api.py
  |-- main.py
  |-- colors.py
  |-- archive/
  |-- output/
  |-- agents/
    |-- math_agent.py
    |-- market_analysis_agent.py
    |-- language_agent.py
    |-- __init__.py
    |-- agent.py
  |-- utilities/
  |-- prompts/
  |-- myenv/
  |-- .git/

Code Blocks:

llm_api.py
```python
import os
import json
from dotenv import load_dotenv
import google.generativeai as genai
from colors import Colors

DEBUG_MODE = True

# Load the environment variables from the .env file
load_dotenv()

# Configure the API with your API key stored in an environment variable
genai.configure(api_key=os.environ["API_KEY"])

# Create a model instance
model = genai.GenerativeModel('gemini-1.0-pro-latest')

def call_google_llm_api(prompt):
    if DEBUG_MODE:
        # print(f"{Colors.GREEN}DEBUG: PROMPT SENT TO GEMINI API for master agent's prompt:\n\n{Colors.END}")
        print(f"{Colors.GREEN}{prompt}{Colors.END}")
        # print("\n\n END OF PROMPT")
    try:
        response = model.generate_content(prompt)
        # Convert the raw text response to JSON format
        json_response = json.dumps({"response": response.text})
        print(f"{Colors.BLUE}{json_response}{Colors.END}")
        return json_response
    except Exception as e:
        print(f"An error occurred: {e}")
        return None
```

main.py
```python
import json
import re
from agents.math_agent import MathAgent
from agents.language_agent import LanguageAgent
from agents.market_analysis_agent import MarketAnalysisAgent
from llm_api import call_google_llm_api
from colors import Colors

# Load prompts from files
with open("prompts/master_agent_prompt.txt", "r") as file:
    master_agent_prompt = file.read()

with open("prompts/math_agent_prompt.txt", "r") as file:
    math_agent_prompt = file.read()

with open("prompts/language_agent_prompt.txt", "r") as file:
    language_agent_prompt = file.read()

# Predefined function table and agent instances
function_table = {
    "calculate": lambda expression: eval(expression),
    "translate": lambda text, target_lang: f"Translating '{text}' to {target_lang}"
}

math_agent = MathAgent("Math Agent", "Mathematics", function_table, math_agent_prompt)
language_agent = LanguageAgent("Language Agent", "Language", function_table, language_agent_prompt)
market_analysis_agent = MarketAnalysisAgent("Market Analysis Agent", "Market Analysis", function_table, "")

agent_registry = {
    "Math Agent": math_agent,
    "Language Agent": language_agent,
    "Market Analysis Agent": market_analysis_agent
}

def extract_provided_info(task_request, required_fields):
    prompt = f"""
    Given the following task request:
    "{task_request}"
    
    Extract the provided information and return a JSON object with the extracted details.
    The JSON object should have keys corresponding to the required information fields, and the values should be the extracted information if available, or an empty string if not provided.
    
    Required information fields:
    {", ".join(required_fields)}
    
    Example:
    Task request: "perform market research for open-world game in southeast asia, focus on demographics age between 18 and 25 years old."
    Required fields: industry, product_service_description, target_market_location, target_market_demographics
    Output:
    {{
        "industry": "video game",
        "product_service_description": "open-world game",
        "target_market_location": "southeast asia",
        "target_market_demographics": "age between 18 and 25 years old"
    }}

    JSON Output:
    """
    response = call_google_llm_api(prompt)
    try:
        provided_info = json.loads(response)
        return provided_info
    except (json.JSONDecodeError, KeyError):
        return {}

def collect_required_info(delegations, user_input):
    required_info = {}
    required_fields = []

    for delegation in delegations:
        agent_name = delegation["agent"]
        agent = agent_registry.get(agent_name)
        if agent:
            agent_required_fields = agent.get_required_info()
            for field in agent_required_fields:
                if field not in required_fields:
                    required_fields.append(field)

    provided_info_response = extract_provided_info(user_input, required_fields)
    provided_info_json = provided_info_response["response"]

    # Use regular expressions to extract the JSON data from the response
    json_match = re.search(r'```\s*(?:JSON|json)\s*(.*?)\s*```', provided_info_json, re.DOTALL)
    if json_match:
        json_data = json_match.group(1)
        # Remove any escape characters and parse the JSON
        json_data = json_data.replace("\\", "")
        provided_info = json.loads(json_data)
        required_info.update(provided_info)

    missing_fields = [field for field in required_fields if required_info.get(field) is None or (isinstance(required_info.get(field), str) and required_info.get(field).strip() == "")]

    if missing_fields:
        print("Please provide the following additional information:")
        for field in missing_fields:
            user_input = input(f"{field}: ")
            required_info[field] = user_input

    return required_info

# Conversation loop
conversation_history = ""
user_input = input("User: ")

while True:
    # Update the conversation history and user input in the master agent's prompt
    prompt = master_agent_prompt.replace("<conversation_history>", conversation_history)
    prompt = prompt.replace("<user_input>", user_input)

    # DEBUG
    print("PROMPT FOR MASTER AGENT:\n\n")

    master_agent_response = call_google_llm_api(prompt)

    if master_agent_response is None:
        print("An error occurred while communicating with the LLM API.")
        break

    # Parse the JSON response
    response_dict = json.loads(master_agent_response)
    response_text = response_dict["response"]
    delegations_dict = json.loads(response_text)
    delegations = delegations_dict["delegations"]

    # Collect and validate the required information for all delegations
    required_info = collect_required_info(delegations, user_input)

    # Process each delegation
    for delegation in delegations:
        agent_name = delegation["agent"]
        task_type = delegation["task"]
        
        # Get the relevant agent
        agent = agent_registry.get(agent_name)
        
        if agent:
            # Filter the required information for the current agent
            agent_required_info = {field: value for field, value in required_info.items() if field in agent.get_required_info()}
            
            # Pass the required information to the agent
            response = agent.generate_response(agent_required_info)
            conversation_history += f"{agent_name}: {response}\n"
        else:
            print()
            conversation_history += f"No agent found for task type: {task_type}\n"

    # Check if all delegations have been completed
    prompt = f"""
Given the following conversation history:

{conversation_history}

Determine if all the delegated tasks have been completed and provide a final response to the user. If the tasks are not yet completed, indicate what additional information or tasks are needed.

Please respond with a well-formatted JSON object using the following structure:
{{
  "tasks_completed": true or false,
  "final_response": "Final response to the user if tasks are completed",
  "additional_info_needed": "Additional information or tasks needed if not completed"
}}

Ensure that the JSON object is complete and properly formatted, without any extra text, formatting characters, or code blocks.

JSON Response:
"""

    master_agent_response = call_google_llm_api(prompt)

    if master_agent_response is None:
        print("An error occurred while communicating with the LLM API.")
        break

    try:
        # Parse the JSON response
        response_dict = json.loads(master_agent_response)
        response_text = response_dict["response"]
        
        # Remove any extra text, formatting characters, or code blocks
        response_text = response_text.strip()
        response_text = response_text.replace("\`\`\`json", "").replace("\`\`\`", "")
        response_text = response_text.strip()

        if response_text.startswith("{") and response_text.endswith("}"):
            result_dict = json.loads(response_text)
            tasks_completed = result_dict.get("tasks_completed", False)
            final_response = result_dict.get("final_response", "")
            additional_info_needed = result_dict.get("additional_info_needed", "")
        else:
            raise json.JSONDecodeError("Invalid JSON format", response_text, 0)

        if tasks_completed:
            conversation_history += f"Master Agent: {final_response}\n"
            print(final_response)
        else:
            conversation_history += f"Master Agent: {additional_info_needed}\n"
            print(additional_info_needed)
    except (json.JSONDecodeError, KeyError) as e:
        print("Error parsing response:", e)
        conversation_history += "Master Agent: I apologize, but I couldn't generate a proper response. Can you please try rephrasing your request?\n"
        print("I apologize, but I couldn't generate a proper response. Can you please try rephrasing your request?")

    # Check if the user wants to end the conversation
    if user_input.lower() in ["bye", "goodbye", "exit", "quit"]:
        break

    # Get the user's input for the next turn
    user_input = input("User: ")

# End the conversation
print("Master Agent: Thank you for the conversation. Have a great day!")
```

colors.py
```python
# colors.py

class Colors:
    GREEN = '\033[92m'  # Green text
    BLUE = '\033[94m'   # Blue text
    END = '\033[0m'     # Reset to default color

```

math_agent.py
```python
import ast
import operator
import math
import json
from .agent import Agent
from llm_api import call_google_llm_api

class MathExpressionEvaluator:
    def __init__(self):
        self.operators = {
            ast.Add: operator.add,
            ast.Sub: operator.sub,
            ast.Mult: operator.mul,
            ast.Div: operator.truediv,
            ast.Pow: operator.pow
        }
        self.functions = {
            'sqrt': math.sqrt,
            'log': math.log,
            'log10': math.log10,
            'log2': math.log2,
            'exp': math.exp,
            'sin': math.sin,
            'cos': math.cos,
            'tan': math.tan,
            'factorial': math.factorial,
            'pow': math.pow  # Add the 'pow' function
        }

    def evaluate(self, expression):
        try:
            tree = ast.parse(expression, mode='eval')
            return self.eval_node(tree.body)
        except SyntaxError:
            # If the expression contains the 'pow()' function, replace it with '**' and try again
            if 'pow(' in expression:
                expression = expression.replace('pow(', '(').replace(',', '**')
                tree = ast.parse(expression, mode='eval')
                return self.eval_node(tree.body)
            else:
                raise ValueError("Invalid mathematical expression")

    def eval_node(self, node):
        if isinstance(node, ast.Num):
            return node.n
        elif isinstance(node, ast.BinOp):
            left = self.eval_node(node.left)
            right = self.eval_node(node.right)
            return self.operators[type(node.op)](left, right)
        elif isinstance(node, ast.UnaryOp):
            operand = self.eval_node(node.operand)
            return self.operators[type(node.op)](operand)
        elif isinstance(node, ast.Call):
            func_name = node.func.id
            if func_name not in self.functions:
                raise ValueError(f"Unknown function: {func_name}")
            args = [self.eval_node(arg) for arg in node.args]
            return self.functions[func_name](*args)
        else:
            raise TypeError(f"Unsupported node type: {type(node)}")

class MathAgent(Agent):
    def __init__(self, name, expertise, function_table, prompt):
        super().__init__(name, expertise, function_table, prompt)
        self.expression_evaluator = MathExpressionEvaluator()

    def generate_response(self, task):
        # Generate a response for math-related tasks
        if "calculate" in task.lower():
            # Convert the natural language task to a mathematical expression
            expression = self.convert_to_expression(task)
            print(f"EXPRESSION: {expression}")

            if expression:
                try:
                    # Evaluate the expression using the custom expression evaluator
                    result = self.expression_evaluator.evaluate(expression)
                    print(f"RESULT: {result}")  # Debugging message
                    return f"The result of {expression} is: {result}"
                except (ValueError, TypeError) as e:
                    print(f"ERROR: {str(e)}")  # Debugging message
                    return str(e)
            else:
                return "I couldn't extract a valid mathematical expression from the task. Please rephrase it."
        else:
            return "I can assist with mathematical calculations. Please provide a valid expression."

    def convert_to_expression(self, task):
        # Call the Google LLM API to convert the natural language task to a mathematical expression
        prompt = f"""
        Convert the following natural language task to a mathematical expression that can be evaluated using the math library in Python. Respond with a JSON object containing the expression:
        Task: {task}
        Respond with a JSON object in the following format:
        {{ "expression": "<mathematical_expression>" }}
        Make sure to include only the JSON object in your response, without any additional text or explanations.
        Examples:
        Task: Calculate the square root of 64
        Response: {{ "expression": "sqrt(64)" }}
        Task: Calculate the natural logarithm of 10
        Response: {{ "expression": "log(10)" }}
        Task: Calculate log base 10 of 100
        Response: {{ "expression": "log10(100)" }}
        JSON Response:
        """
        response = call_google_llm_api(prompt)

        if response:
            try:
                response_data = json.loads(response)
                expression_data = json.loads(response_data["response"])
                return expression_data["expression"]
            except (json.JSONDecodeError, KeyError):
                return None
        else:
            return None
```

market_analysis_agent.py
```python
from .agent import Agent

class MarketAnalysisAgent(Agent):
    def get_required_info(self):
        return [
            "industry",
            "product_service_description",
            "target_market_location",
            "target_market_demographics",
            "known_competitors",
            "competitive_factors",
            "emerging_trends",
            "regulatory_considerations",
            "additional_notes"
        ]

    def generate_response(self, task_info):
        print("GENERATING RESPONSE...")
        # Perform market analysis based on the provided task_info
        # ...
        response = f"Market Analysis Results:\n{task_info}"
        return response
```

language_agent.py
```python
from .agent import Agent

class LanguageAgent(Agent):
    def generate_response(self, task):
        # Generate a response for language-related tasks
        if "translate" in task.lower():
            # Extract the text and target language from the task and translate
            parts = task.split("translate", 1)[1].strip().split("to")
            text = parts[0].strip().strip("'\"")
            target_lang = parts[1].strip()
            translated_text = self.execute_function("translate", text, target_lang)
            return f"The translation of '{text}' to {target_lang} is: {translated_text}"
        else:
            return "I can assist with language translation. Please provide the text and target language."
```

__init__.py
```python

```

agent.py
```python
class Agent:
    def __init__(self, name, expertise, function_table, prompt):
        self.name = name
        self.expertise = expertise
        self.function_table = function_table
        self.prompt = prompt

    def execute_function(self, function_name, *args, **kwargs):
        if function_name in self.function_table:
            return self.function_table[function_name](*args, **kwargs)
        else:
            raise ValueError(f"Function '{function_name}' not found in the function table")

    def generate_response(self, task):
        # Process the task and generate a response
        # This method can be overridden by specialized agents
        return "I'm not sure how to handle this task."
```

Prompts:

master_agent_prompt.txt

```
[System Description]
You are a master agent responsible for coordinating and delegating tasks to specialized agents based on the user's input. Your role is to analyze the input, determine which agent(s) are relevant for the task, and provide a JSON response containing only the delegation instructions.

[Input Format]
The user input will be provided in the following format:
User: <user_input>

[Output Format]
Your response should be a JSON object in the following format:
{
  "delegations": [
    {
      "agent": "<agent_name>",
      "task": "<task_description>"
    },
    ...
  ]
}

- "delegations" is an array of delegation objects.
- Each delegation object contains the "agent" (name of the specialized agent) and the "task" (description of the task to be performed by the agent).
- If no delegation is required, the "delegations" array should be empty.
- Do not include any other text, conversation, or explanations in your response, only the JSON object.

[Available Agents]
- Math Agent: Responsible for mathematical calculations and expressions.
- Language Agent: Responsible for language translation and related tasks.
- Market Analysis Agent: Responsible to perform market analysis.

[Example 1]
User: Can you translate "Hello" to Spanish?

{"delegations":[{"agent":"Language Agent","task":"Translate 'Hello' to Spanish"}]}

[Example 2]
User: What is the result of 2 + 3?

{"delegations":[{"agent":"Math Agent","task":"Calculate 2 + 3"}]}

[Example 3]
User: Please send an email to John.

{"delegations":[]}

[Current Conversation]
<conversation_history>

[User Input]
<user_input>

[Instructions]
Based on the current conversation history and the user's latest input, generate a JSON response containing only the delegation instructions for the relevant agent(s). Do not include any other text, conversation, or explanations in your response.
```

math_agent_prompt.txt
```
[System Description]
You are a math agent responsible for performing mathematical calculations and evaluating expressions.

[Input Format]
The task will be provided in the following format:
Task: <task_description>

[Output Format]
Your response should be a plain text string containing the result of the calculation or an appropriate message.

[Examples]
Task: Calculate 2 + 3
Response: The result of 2 + 3 is: 5

Task: Calculate the square root of 64
Response: The result of the square root of 64 is: 8.0

Task: Solve the equation x + 5 = 10
Response: I can assist with mathematical calculations. Please provide a valid expression.
```